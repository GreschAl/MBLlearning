{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases the usage of the `MBLlearning`-package.\n",
    "It loads all necessary data and the pretrained model(s) but can also incorporate training the model from scratch.\n",
    "All follow-up code is used to recreate every plot from the result section in the main text of the arXiv-paper\n",
    "\n",
    "For additional plots in the appendix (especially the toy model), refer to the `toy-example`-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant packages, in particular, the MBLlearning-package\n",
    "# load pytorch for training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# load finite-size scaling analysis package\n",
    "import fssa\n",
    "# standard libraries for data processing and plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import axes3d # for 3d plots\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MBLlearning.learn_inds.approximation import Approximator\n",
    "from MBLlearning.learn_inds import recurrent\n",
    "import MBLlearning.utils as utils\n",
    "import MBLlearning.utils.plotting as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MBLlearning.global_config import get_config, diction\n",
    "DTYPE, DEVICE = get_config()\n",
    "NUM_DEVICES = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "print(\"Using {} device(s) of type {}\".format(NUM_DEVICES,DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the RNN-set-up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all relevant hyperparameters and data setting go here\n",
    "\n",
    "################################################################################################################\n",
    "# data settings\n",
    "fileloc = \"data/indicators_train_Uniform_L_{}_eps_{}.txt\"\n",
    "L = [10,12,14]\n",
    "L_small = [10,12] # for quantitative extrapolation testing\n",
    "L_extra = [16,18] # for qualitative  extrapolation testing\n",
    "eps = np.arange(1,20)/20\n",
    "single_inds = [0,3,5]\n",
    "######## transfer learning indicators #######\n",
    "init_inds = [0,1]\n",
    "transfer_inds = [2]\n",
    "#############################################\n",
    "sorting = False\n",
    "N_train = 1000\n",
    "################################################################################################################\n",
    "# hyperparameters of the model\n",
    "feature_size = 8 # single number for RNN feature extractor\n",
    "depth = 1 # depth of RNN-cell, see documentation of nn.GRU or nn.LSTM\n",
    "use_LSTM = False # switch between GRU or LSTM cell\n",
    "hidden_size = [10] # array of hidden dimensions for the hidden layer for fully-connected NN or None-Type\n",
    "seq_division = 2 # preprocessing tuple size e.g. [h1,h2,h3] --> [(h1,h2),(h2,h3),(h3,h1)]\n",
    "# training procedure\n",
    "optimizer = optim.Adam\n",
    "opt_params = (1e-4,) # learning rate, other parameters left as default\n",
    "N_epochs = 20\n",
    "batch_size = 32 # batch size per device\n",
    "model_name_scheme = \"trained_models/large_model_{}_hidden_size_{}_{}.pt\" # {full, small, switched} x {rnn, model}\n",
    "                                                                         # to be filled in\n",
    "retrain_model = False # checks for availability of existing model file first.\n",
    "                      # Can be suppressed by setting this to True\n",
    "retrain_N_trains = False # setting this to True reruns the generation of data for plot 4 in the paper\n",
    "                         # it is very time-consuming though\n",
    "N_trains_retrain = np.append(np.arange(1,11),[10,100,1000])\n",
    "retrain_filename = \"mse_N_train/{}_loss_L_{}_N_{}.txt\"\n",
    "N_trains_plot = np.array([1,2,4,10,100,1000])\n",
    "mute_outputs = False  # verbose level during training\n",
    "################################################################################################################\n",
    "# save figures? Provide a folder for the images\n",
    "save_figures = \"plots_paper/\" # set to None-Type for no saving of plots\n",
    "################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "neural_net = Approximator(L,energies=eps)\n",
    "# data loading and model init\n",
    "neural_net.set_up(fileloc,\n",
    "                  rnn_params=(feature_size,depth),\n",
    "                  model_params=hidden_size,\n",
    "                  seq_division=seq_division,\n",
    "                  use_LSTM=use_LSTM,\n",
    "                  learn_single_ind=single_inds,\n",
    "                  N_train=N_train)\n",
    "# resets the model parameters but can also be used for altering the model architecture\n",
    "neural_net.reset_model()\n",
    "# regardless of training, set the optimizer and its parameters\n",
    "neural_net.optimizer = optimizer\n",
    "neural_net.opt_params = opt_params\n",
    "\n",
    "print()\n",
    "print(\"Hidden size =\",hidden_size[0])\n",
    "model_name = model_name_scheme.format(\"full\",hidden_size[0],\"{}\") # fill in first place-holder\n",
    "neural_net.reset_model()\n",
    "if retrain_model:\n",
    "    print(\"Retraining model from scratch ...\")\n",
    "    neural_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs)\n",
    "    print(\"Retraining finished, saving to file.\")\n",
    "    neural_net.save(model_name)\n",
    "else:\n",
    "    print(\"Trying to load model from file ...\")\n",
    "    try:\n",
    "        neural_net.load(model_name)\n",
    "    except:\n",
    "        print(\"File not found, training the model ...\")\n",
    "        neural_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs)\n",
    "        print(\"Training finished, saving to file.\")\n",
    "        neural_net.save(model_name)\n",
    "    else:\n",
    "        print(\"Model loaded from file.\")\n",
    "\n",
    "# print the number of parameters in the RNN and the consecutive model\n",
    "neural_net.num_params()\n",
    "\n",
    "# create plot 3 in paper\n",
    "temp = None if save_figures is None else save_figures + \"indicator_plot_{}\"\n",
    "\n",
    "plot.plot_inds(neural_net,label_idxs=single_inds,savename=temp)\n",
    "\n",
    "print(neural_net.Lvals)\n",
    "plot.plot_inds_extrapolation(neural_net,[10,12,14,16,18,20,30],eps,label_idxs=single_inds)\n",
    "print(neural_net.Lvals)\n",
    "\n",
    "# check whether to retrain data with reduced N_train but increased N_epochs\n",
    "# by default, load from file\n",
    "if retrain_N_trains:\n",
    "    N_max = max(N_trains_retrain)\n",
    "    num_reps = 2\n",
    "\n",
    "    #mses = np.zeros((num_reps,len(N_trains),len(L)))\n",
    "    losses = []\n",
    "\n",
    "    for i,N in enumerate(N_trains_retrain):\n",
    "        # set up network\n",
    "        temp_losses = []\n",
    "        temp_net = Approximator(L,energies=eps)\n",
    "        temp_net.set_up(fileloc,\n",
    "                        rnn_params=(feature_size,depth),\n",
    "                        model_params=hidden_sizes,\n",
    "                        seq_division=seq_division,\n",
    "                        use_LSTM=use_LSTM,\n",
    "                        learn_single_ind=single_inds,\n",
    "                        N_train=N)\n",
    "\n",
    "        temp_net.optimizer = optimizer\n",
    "        temp_net.opt_params = opt_params\n",
    "\n",
    "        for k in range(num_reps):            \n",
    "            temp = np.infty\n",
    "            while temp > 1:\n",
    "                # retry if training does not succeed which is visible in a high loss\n",
    "                temp_net.reset_model()\n",
    "                temp_net.Lvals = L_small # train only on smaller data file\n",
    "                temp_epochs = int(N_epochs*N_max/N)\n",
    "                temp_N = len(temp_net.data[L[0]][\"train\"][\"hcorr\"])\n",
    "                print(\"Training with {} data points for {} epochs\".format(temp_N,temp_epochs))\n",
    "                _, loss_curve = temp_net.train(temp_epochs,batch_size*NUM_DEVICES,mute_outputs=True,L_tests=L)\n",
    "                temp_net.Lvals = L # test on all test data\n",
    "                temp = np.mean(loss_curve[0,-1]) # retry if final loss is above threshold\n",
    "                #for j,l in enumerate(L):\n",
    "                #    h, singlescores, targets = temp_net.predict(l)\n",
    "                #    mse = ((singlescores-targets)**2).mean()\n",
    "                #    mses[k,i,j] = mse\n",
    "                #    print(\"MSE on L={} on the test set was {}\".format(l,mse))\n",
    "                #    temp = mse # retry if too large\n",
    "            temp_losses.append(loss_curve)\n",
    "        #mse_avg = np.mean(mses,axis=0)\n",
    "        #mse_std = np.std(mses,axis=0)/np.sqrt(num_reps)\n",
    "        losses.append(temp_losses)\n",
    "    \n",
    "    # save all losses\n",
    "    for j,l in enumerate(L):\n",
    "        for k,key in enumerate([\"train\",\"test\"]):\n",
    "            for loss,N in zip(losses,N_trains):\n",
    "                np.savetxt(retrain_filename.format(key,l,N),np.array(loss)[:,k,:,j].T)\n",
    "                print(np.array(loss)[:,k,:,j].T.shape)\n",
    "\n",
    "    \n",
    "prefactor = len(neural_net.data[L[0]][\"train\"][\"h\"])*len(L_small)/batch_size/NUM_DEVICES\n",
    "# create plot 4 in paper\n",
    "temp = None if save_figures is None else save_figures + \"N_trains_plot_{}\"\n",
    "plot.plot_N_train_losses(12,N_trains_plot,N_epochs,prefactor,retrain_filename,savename=temp)\n",
    "plot.plot_N_train_losses([10,14],N_trains_plot,N_epochs,prefactor,retrain_filename,savename=temp)\n",
    "\n",
    "if False:\n",
    "    # scatter plot losses\n",
    "    color = [\"red\",\"blue\",\"green\"]\n",
    "    temp = len(L)*[None]\n",
    "    for i,(data,c,l) in enumerate(zip(mses.T,color,L)):\n",
    "        #mean,std = filtered_statistics(data,threshold=1000)\n",
    "        for N,dots in zip(N_trains,data):\n",
    "            temp[i] = plt.scatter(N*np.ones_like(dots),dots,c=c)\n",
    "    plt.legend(temp,L)\n",
    "    plt.xscale('log')\n",
    "    plt.show()    \n",
    "\n",
    "    # create plot 4 in paper\n",
    "    linestyles = [\"bx\",\"kx\",None]\n",
    "    for l,avg,std,style in zip(L,mse_avg.T,mse_std.T,linestyles):\n",
    "        if l==14:\n",
    "            plt.semilogx(N_trains,avg,color=\"orange\",marker=\"*\",label=\"L={}*\".format(l))\n",
    "            plt.errorbar(N_trains,avg,yerr=std,fmt=\"none\",ecolor=\"orange\")\n",
    "        else:\n",
    "            plt.semilogx(N_trains,avg,style,label=\"L={}\".format(l))\n",
    "            plt.errorbar(N_trains,avg,yerr=std,fmt=\"none\",ecolor=style[0])\n",
    "    plt.legend(fontsize=\"x-large\")\n",
    "    plt.xticks(fontsize=\"x-large\")\n",
    "    plt.yticks(fontsize=\"x-large\")\n",
    "    plt.ylabel(\"Mean MSE\",fontsize=\"x-large\")\n",
    "    plt.xlabel(\"$N_{train}$ per disorder parameter $h$\",fontsize=\"x-large\")\n",
    "    if save_figures is not None:\n",
    "        plt.savefig(save_figures+\"mse_N_train.pdf\",orientation=\"landscape\",dpi=600,bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipped old code. Check for deletion.\")\n",
    "\n",
    "# checks whether to (re)train the model or to load from file on smaller training data\n",
    "model_name = model_name_scheme.format(\"small\",hidden_size[0],\"{}\") # fill in first place-holder\n",
    "if retrain_model:\n",
    "    neural_net.Lvals = L_small # for training purposes\n",
    "    print(\"Retraining model from scratch ...\")\n",
    "    neural_net.reset_model()\n",
    "    neural_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs)\n",
    "    print(\"Retraining finished, saving to file.\")\n",
    "    neural_net.save(model_name)\n",
    "    neural_net.Lvals = L # for full test set analysis\n",
    "else:\n",
    "    print(\"Trying to load model from file ...\")\n",
    "    try:\n",
    "        neural_net.load(model_name)\n",
    "    except:\n",
    "        neural_net.Lvals = L_small # for training purposes\n",
    "        print(\"File not found, training the model ...\")\n",
    "        neural_net.reset_model()\n",
    "        neural_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs)\n",
    "        print(\"Training finished, saving to file.\")\n",
    "        neural_net.save(model_name)\n",
    "        neural_net.Lvals = L # for full test set analysis\n",
    "    else:\n",
    "        print(\"Model loaded from file.\")\n",
    "\n",
    "# coefficient of determination on smaller training set (plot 5 in paper)\n",
    "temp = None if save_figures is None else save_figures + \"coefficient_of_determination\"\n",
    "fit_params = plot.plot_r2(neural_net,label_idxs=single_inds,savename=temp)\n",
    "\n",
    "# FSSA\n",
    "neural_net.swap_datasets()\n",
    "neural_net.reset_model()\n",
    "# check whether to retrain or load from file\n",
    "model_name = model_name_scheme.format(\"fssa\",hidden_size[0],\"{}\") # fill in first place-holder\n",
    "if retrain_model:\n",
    "    neural_net.Lvals = L_small # for training purposes\n",
    "    neural_net.reset_model()\n",
    "    print(\"Retraining model from scratch ...\")\n",
    "    neural_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs)\n",
    "    print(\"Retraining finished, saving to file.\")\n",
    "    neural_net.save(model_name)\n",
    "    neural_net.Lvals = L # for full test set analysis\n",
    "else:\n",
    "    print(\"Trying to load model from file ...\")\n",
    "    try:\n",
    "        neural_net.load(model_name)\n",
    "    except:\n",
    "        neural_net.Lvals = L_small # for training purposes\n",
    "        print(\"File not found, training the model ...\")\n",
    "        neural_net.reset_model()\n",
    "        neural_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs)\n",
    "        print(\"Training finished, saving to file.\")\n",
    "        neural_net.save(model_name)\n",
    "        neural_net.Lvals = L # for full test set analysis\n",
    "    else:\n",
    "        print(\"Model loaded from file.\")\n",
    "\n",
    "# create plot 6 in paper\n",
    "temp = None if save_figures is None else save_figures + \"fssa_consistency\"\n",
    "fit_params = plot.plot_vanilla_fssa(neural_net,label_idxs=single_inds,savename=temp)\n",
    "\n",
    "# create plot 7 in paper\n",
    "temp = None if save_figures is None else save_figures + \"fssa_extrapolate\"\n",
    "fit_params = plot.plot_extrapolated_fssa(neural_net,label_idxs=single_inds,savename=temp)\n",
    "\n",
    "# transfer learning. Train on two indicators first, then drop the post-processing model\n",
    "# Finally, retrain on the third indicator\n",
    "# Reset data mess from FSSA\n",
    "transfer_net = Approximator(L,energies=eps)\n",
    "# data loading and model init\n",
    "transfer_net.set_up(fileloc,\n",
    "                  rnn_params=(feature_size,depth),\n",
    "                  model_params=hidden_size,\n",
    "                  seq_division=seq_division,\n",
    "                  use_LSTM=use_LSTM,\n",
    "                  learn_single_ind=single_inds,\n",
    "                  N_train=N_train)\n",
    "transfer_net.reset_model()\n",
    "transfer_net.optimizer = optimizer\n",
    "transfer_net.opt_params = opt_params\n",
    "# check whether to retrain or load from file for initial training\n",
    "model_name = model_name_scheme.format(\"transfer_init\",hidden_size[0],\"{}\") # fill in first place-holder\n",
    "if retrain_model:\n",
    "    transfer_net.Lvals = L_small # for training purposes\n",
    "    print(\"Retraining model from scratch ...\")\n",
    "    transfer_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs,\n",
    "                     single_inds=init_inds\n",
    "                    )\n",
    "    print(\"Retraining finished, saving to file.\")\n",
    "    transfer_net.save(model_name)\n",
    "    transfer_net.Lvals = L # for full test set analysis\n",
    "else:\n",
    "    print(\"Trying to load model from file ...\")\n",
    "    N_inds = transfer_net.N_inds\n",
    "    try:\n",
    "        # juggle with the number of indicators for loading procedure\n",
    "        transfer_net.N_inds = len(init_inds)\n",
    "        transfer_net.reset_model()\n",
    "        transfer_net.load(model_name)\n",
    "    except:\n",
    "        transfer_net.N_inds = N_inds\n",
    "        transfer_net.Lvals = L_small # for training purposes\n",
    "        print(\"File not found, training the model ...\")\n",
    "        transfer_net.train(epochs=N_epochs,\n",
    "                         batch_size=batch_size*NUM_DEVICES,\n",
    "                         mute_outputs=mute_outputs,\n",
    "                         single_inds=init_inds\n",
    "                        )\n",
    "        print(\"Training finished, saving to file.\")\n",
    "        transfer_net.save(model_name)\n",
    "        transfer_net.Lvals = L # for full test set analysis\n",
    "    else:\n",
    "        transfer_net.N_inds = N_inds\n",
    "        print(\"Model loaded from file.\")\n",
    "# check whether to retrain or load from file for transfer training\n",
    "model_name = model_name_scheme.format(\"transfer\",hidden_size[0],\"{}\") # fill in first place-holder\n",
    "if retrain_model:\n",
    "    transfer_net.Lvals = L_small # for training purposes\n",
    "    print(\"Retraining model from scratch ...\")\n",
    "    transfer_net.train_transfer(ind_idxs=transfer_inds,\n",
    "                     epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs\n",
    "                    )\n",
    "    print(\"Retraining finished, saving to file.\")\n",
    "    transfer_net.save(model_name)\n",
    "    transfer_net.Lvals = L # for full test set analysis\n",
    "else:\n",
    "    print(\"Trying to load model from file ...\")\n",
    "    N_inds = transfer_net.N_inds\n",
    "    try:\n",
    "        # juggle with the number of indicators for loading procedure\n",
    "        transfer_net.N_inds = len(transfer_inds)\n",
    "        transfer_net.reset_model()\n",
    "        transfer_net.load(model_name)\n",
    "    except:\n",
    "        transfer_net.N_inds = N_inds\n",
    "        transfer_net.Lvals = L_small # for training purposes\n",
    "        print(\"File not found, training the model ...\")\n",
    "        transfer_net.train_transfer(ind_idxs=transfer_inds,\n",
    "                         epochs=N_epochs,\n",
    "                         batch_size=batch_size*NUM_DEVICES,\n",
    "                         mute_outputs=mute_outputs\n",
    "                        )\n",
    "        print(\"Training finished, saving to file.\")\n",
    "        transfer_net.save(model_name)\n",
    "        transfer_net.Lvals = L # for full test set analysis\n",
    "    else:\n",
    "        transfer_net.N_inds = N_inds\n",
    "        print(\"Model loaded from file.\")\n",
    "\n",
    "# set-up adversial network that learns the transfer indicator from scratch\n",
    "adversary_net = Approximator(L,energies=eps)\n",
    "# data loading and model init\n",
    "adversary_net.set_up(fileloc,\n",
    "                  rnn_params=(feature_size,depth),\n",
    "                  model_params=hidden_size,\n",
    "                  seq_division=seq_division,\n",
    "                  use_LSTM=use_LSTM,\n",
    "                  learn_single_ind=[single_inds[t] for t in transfer_inds],\n",
    "                  N_train=N_train)\n",
    "adversary_net.reset_model()\n",
    "adversary_net.optimizer = optimizer\n",
    "adversary_net.opt_params = opt_params\n",
    "# check whether to retrain or load from file for initial training\n",
    "model_name = model_name_scheme.format(\"transfer_adversary\",hidden_size[0],\"{}\") # fill in first place-holder\n",
    "if retrain_model:\n",
    "    adversary_net.Lvals = L_small # for training purposes\n",
    "    print(\"Retraining model from scratch ...\")\n",
    "    adversary_net.train(epochs=N_epochs,\n",
    "                     batch_size=batch_size*NUM_DEVICES,\n",
    "                     mute_outputs=mute_outputs\n",
    "                    )\n",
    "    print(\"Retraining finished, saving to file.\")\n",
    "    adversary_net.save(model_name)\n",
    "    adversary_net.Lvals = L # for full test set analysis\n",
    "else:\n",
    "    print(\"Trying to load model from file ...\")\n",
    "    try:\n",
    "        adversary_net.load(model_name)\n",
    "    except:\n",
    "        adversary_net.Lvals = L_small # for training purposes\n",
    "        print(\"File not found, training the model ...\")\n",
    "        adversary_net.train(epochs=N_epochs,\n",
    "                         batch_size=batch_size*NUM_DEVICES,\n",
    "                         mute_outputs=mute_outputs\n",
    "                        )\n",
    "        print(\"Training finished, saving to file.\")\n",
    "        adversary_net.save(model_name)\n",
    "        adversary_net.Lvals = L # for full test set analysis\n",
    "    else:\n",
    "        print(\"Model loaded from file.\")\n",
    "\n",
    "# reswap data sets for comparison\n",
    "neural_net.swap_datasets()\n",
    "# create plot 8 in paper concerning transfer learning\n",
    "temp = None if save_figures is None else save_figures + \"transfer_learning\"\n",
    "plot.plot_r2_comparison(transfer_net,neural_net,adversary_net,transfer_inds,savename=temp)\n",
    "\n",
    "# create plot 11 from appendix\n",
    "temp = None if save_figures is None else save_figures + \"l_dependence\"\n",
    "plot.plot_L_dependent_regression(neural_net,label_idxs=single_inds,savename=temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
